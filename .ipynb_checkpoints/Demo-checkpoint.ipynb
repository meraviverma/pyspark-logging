{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, concat_ws, lit\n",
    "import os,sys\n",
    "from dependencies.spark import start_spark\n",
    "#from os import System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_HOME'] =\"C://Users//RaviVerma//Desktop//software//hadoop\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "os.setProperty(\"hadoop.home.dir\", \"C:\\\\Users\\\\RaviVerma\\\\Desktop\\\\software\\\\hadoop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main ETL script definition.\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # start Spark application and get Spark session, logger and config\n",
    "    spark, log, config = start_spark(\n",
    "        app_name='my_etl_job',\n",
    "        files=['configs/etl_config.json'])\n",
    "    spark.sparkContext.addFile('log4j.properties')\n",
    "\n",
    "    # log that main ETL job is starting\n",
    "    log.error('etl_job is up-and-running')\n",
    "    log.warn('Job started')\n",
    "\n",
    "    # execute ETL pipeline\n",
    "    try:\n",
    "        data = extract_data(spark)\n",
    "        data_transformed = transform_data(data1, config['steps_per_floor'])\n",
    "        load_data(data_transformed)\n",
    "        \n",
    "    except Exception as ex:\n",
    "        log.error(\"Exception occured with dataframe: {}\".format(str(ex)))\n",
    "        spark.stop()\n",
    "    \n",
    "    # log the success and terminate Spark application\n",
    "    log.warn('test_etl_job is finished')\n",
    "    spark.stop()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(spark):\n",
    "    \"\"\"Load data from Parquet file format.\n",
    "\n",
    "    :param spark: Spark session object.\n",
    "    :return: Spark DataFrame.\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        spark\n",
    "        .read\n",
    "        .parquet('tests/test_data/employees'))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df, steps_per_floor_):\n",
    "    \"\"\"Transform original dataset.\n",
    "\n",
    "    :param df: Input DataFrame.\n",
    "    :param steps_per_floor_: The number of steps per-floor at 43 Tanner\n",
    "        Street.\n",
    "    :return: Transformed DataFrame.\n",
    "    \"\"\"\n",
    "    df_transformed = (\n",
    "        df\n",
    "        .select(\n",
    "            col('id'),\n",
    "            concat_ws(\n",
    "                ' ',\n",
    "                col('first_name'),\n",
    "                col('second_name')).alias('name'),\n",
    "               (col('floor') * lit(steps_per_floor_)).alias('steps_to_desk')))\n",
    "\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df):\n",
    "    \"\"\"Collect data locally and write to CSV.\n",
    "\n",
    "    :param df: DataFrame to print.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    df.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data(spark, config):\n",
    "    \"\"\"Create test data.\n",
    "\n",
    "    This function creates both both pre- and post- transformation data\n",
    "    saved as Parquet files in tests/test_data. This will be used for\n",
    "    unit tests as well as to load as part of the example ETL job.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # create example data from scratch\n",
    "    local_records = [\n",
    "        Row(id=1, first_name='Dan', second_name='Germain', floor=1),\n",
    "        Row(id=2, first_name='Dan', second_name='Sommerville', floor=1),\n",
    "        Row(id=3, first_name='Alex', second_name='Ioannides', floor=2),\n",
    "        Row(id=4, first_name='Ken', second_name='Lai', floor=2),\n",
    "        Row(id=5, first_name='Stu', second_name='White', floor=3),\n",
    "        Row(id=6, first_name='Mark', second_name='Sweeting', floor=3),\n",
    "        Row(id=7, first_name='Phil', second_name='Bird', floor=4),\n",
    "        Row(id=8, first_name='Kim', second_name='Suter', floor=4)\n",
    "    ]\n",
    "\n",
    "    df = spark.createDataFrame(local_records)\n",
    "\n",
    "    # write to Parquet file format\n",
    "    (df\n",
    "     .coalesce(1)\n",
    "     .write\n",
    "     .parquet('tests/test_data/employees', mode='overwrite'))\n",
    "\n",
    "    # create transformed version of data\n",
    "    df_tf = transform_data(df, config['steps_per_floor'])\n",
    "\n",
    "    # write transformed version of data to Parquet\n",
    "    (df_tf\n",
    "     .coalesce(1)\n",
    "     .write\n",
    "     .parquet('tests/test_data/employees_report', mode='overwrite'))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# entry point for PySpark ETL application\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    except Exception as e:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "        data = extract_data(spark)\n",
    "        data_transformed = transform_data(data1, config['steps_per_floor'])\n",
    "        load_data(data_transformed)\n",
    "        \n",
    "    except Exception as e:\n",
    "        log.warn(\"Cannot resolve conflicting burrito criteria: {}\".format(e.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
